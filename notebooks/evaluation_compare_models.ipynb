{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation: Compare Results Across Models\n",
    "\n",
    "This notebook loads saved results from **SVM**, **LSTM**, and **GRU** (and optionally other models) and produces comparison tables and figures for the report.\n",
    "\n",
    "**Data source:** `outputs/tables/results_<model>_<embedding>.json`  \n",
    "Run from **project root** so paths resolve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Path to results (relative to project root)\n",
    "TABLES_DIR = 'outputs/tables'\n",
    "if not os.path.isdir(TABLES_DIR):\n",
    "    TABLES_DIR = '../outputs/tables'\n",
    "assert os.path.isdir(TABLES_DIR), f\"Expected {TABLES_DIR} to exist. Run from project root.\"\n",
    "print(f\"Using results from: {os.path.abspath(TABLES_DIR)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load all result JSONs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = []\n",
    "for f in sorted(os.listdir(TABLES_DIR)):\n",
    "    if not f.endswith('.json'):\n",
    "        continue\n",
    "    path = os.path.join(TABLES_DIR, f)\n",
    "    with open(path, 'r') as fp:\n",
    "        d = json.load(fp)\n",
    "    # Normalize embedding name (e.g. Skipgram -> Skip-gram)\n",
    "    emb = d.get('embedding', '')\n",
    "    if emb == 'Skipgram':\n",
    "        emb = 'Skip-gram'\n",
    "    rows.append({\n",
    "        'model': d.get('model', ''),\n",
    "        'embedding': emb,\n",
    "        'accuracy': d.get('accuracy'),\n",
    "        'precision_macro': d.get('precision_macro'),\n",
    "        'recall_macro': d.get('recall_macro'),\n",
    "        'f1_macro': d.get('f1_macro'),\n",
    "        'train_time_sec': d.get('train_time_sec'),\n",
    "    })\n",
    "\n",
    "df = pd.DataFrame(rows)\n",
    "print(f\"Loaded {len(df)} result rows.\")\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comparison table (all models × embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot: rows = model, columns = embedding, values = accuracy\n",
    "acc_pivot = df.pivot_table(index='model', columns='embedding', values='accuracy')\n",
    "print(\"Accuracy by Model and Embedding\")\n",
    "print(acc_pivot.round(4).to_string())\n",
    "print()\n",
    "\n",
    "if df['f1_macro'].notna().any():\n",
    "    f1_pivot = df.pivot_table(index='model', columns='embedding', values='f1_macro')\n",
    "    print(\"F1 (macro) by Model and Embedding\")\n",
    "    print(f1_pivot.round(4).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Bar charts: Accuracy by embedding (grouped by model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = df['model'].unique().tolist()\n",
    "embeddings = df['embedding'].unique().tolist()\n",
    "x = np.arange(len(embeddings))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for i, mod in enumerate(models):\n",
    "    vals = [df[(df['model'] == mod) & (df['embedding'] == e)]['accuracy'].values[0] if len(df[(df['model'] == mod) & (df['embedding'] == e)]) else np.nan for e in embeddings]\n",
    "    ax.bar(x + i * width, vals, width, label=mod)\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(embeddings, rotation=15, ha='right')\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Test accuracy by embedding and model')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Bar charts: Accuracy by model (grouped by embedding)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(len(models))\n",
    "width = 0.25\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "for i, emb in enumerate(embeddings):\n",
    "    vals = [df[(df['model'] == m) & (df['embedding'] == emb)]['accuracy'].values[0] if len(df[(df['model'] == m) & (df['embedding'] == emb)]) else np.nan for m in models]\n",
    "    ax.bar(x + i * width, vals, width, label=emb)\n",
    "ax.set_xticks(x + width)\n",
    "ax.set_xticklabels(models)\n",
    "ax.set_ylabel('Accuracy')\n",
    "ax.set_title('Test accuracy by model and embedding')\n",
    "ax.legend()\n",
    "ax.set_ylim(0, 1.05)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Heatmap: Model × Embedding (accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(8, 4))\n",
    "acc_pivot = df.pivot_table(index='model', columns='embedding', values='accuracy')\n",
    "sns.heatmap(acc_pivot, annot=True, fmt='.3f', cmap='RdYlGn', vmin=0.3, vmax=1.0, ax=ax)\n",
    "ax.set_title('Accuracy: model × embedding')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Summary table (for report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = df.pivot_table(index=['model', 'embedding'], values=['accuracy', 'f1_macro'], aggfunc='first').reset_index()\n",
    "summary = summary.round(4)\n",
    "print(\"Full comparison (copy to report):\")\n",
    "display(summary)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
