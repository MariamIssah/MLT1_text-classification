{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Text Classification with SVM (Kaggle Version)\n",
        "\n",
        "**Self-contained notebook for Kaggle** — no external `embeddings/` or `utils/` files. All preprocessing and TF-IDF / Skip-gram / CBOW code is inlined. Uses Kaggle's preinstalled libraries (pandas, sklearn, nltk, gensim).\n",
        "\n",
        "- **Embeddings**: TF-IDF, Skip-gram (Word2Vec), CBOW (Word2Vec) — defined in this notebook\n",
        "- **Dataset**: Add your dataset in Kaggle and set `DATA_PATH` in the next cell\n",
        "- **Text column**: Product Title  \n",
        "- **Label column**: Cluster Label"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Kaggle: Set path to dataset CSV\n",
        "# After adding dataset to notebook: Add Data -> Your dataset -> copy path\n",
        "import os\n",
        "\n",
        "DATA_PATH = '/kaggle/input/pricerunner-aggregate-csv/pricerunner_aggregate.csv'\n",
        "\n",
        "# If running locally, use local path\n",
        "if not os.path.exists(DATA_PATH):\n",
        "    DATA_PATH = 'data/pricerunner_aggregate.csv'\n",
        "print(f\"Using data: {DATA_PATH}\")\n",
        "\n",
        "# Speed: set True for much faster run (~10–20 min). False = full data & grid (~hours)\n",
        "FAST_MODE = True\n",
        "MAX_CLASSES = 300       # use top N classes by frequency (full data has ~7800)\n",
        "MAX_TRAIN_SAMPLES = 8000   # cap training size when FAST_MODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Download NLTK data (Kaggle has NLTK; may need punkt_tab)\n",
        "import nltk\n",
        "\n",
        "for name in ['punkt_tab', 'punkt', 'stopwords', 'wordnet']:\n",
        "    try:\n",
        "        nltk.download(name, quiet=True)\n",
        "        print(f\"✓ {name}\")\n",
        "    except Exception as e:\n",
        "        print(f\"  {name}: {e}\")\n",
        "print(\"NLTK ready.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports (all available on Kaggle)\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import re\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "from gensim.models import Word2Vec\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "print(\"All libraries imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Text Preprocessor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TextPreprocessor:\n",
        "    def __init__(self, min_word_length=2, remove_stopwords=True, lemmatize=True):\n",
        "        self.min_word_length = min_word_length\n",
        "        self.remove_stopwords = remove_stopwords\n",
        "        self.lemmatize = lemmatize\n",
        "        self.stop_words = set(stopwords.words('english')) if remove_stopwords else set()\n",
        "        self.lemmatizer = WordNetLemmatizer() if lemmatize else None\n",
        "\n",
        "    def clean_text(self, text):\n",
        "        if pd.isna(text):\n",
        "            return \"\"\n",
        "        text = str(text).lower()\n",
        "        text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text, flags=re.MULTILINE)\n",
        "        text = re.sub(r'\\S+@\\S+', '', text)\n",
        "        text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
        "        text = re.sub(r'\\s+', ' ', text).strip()\n",
        "        return text\n",
        "\n",
        "    def tokenize(self, text):\n",
        "        return word_tokenize(text)\n",
        "\n",
        "    def preprocess(self, text):\n",
        "        cleaned = self.clean_text(text)\n",
        "        tokens = self.tokenize(cleaned)\n",
        "        filtered = [t for t in tokens if len(t) >= self.min_word_length\n",
        "                    and (not self.remove_stopwords or t not in self.stop_words)]\n",
        "        if self.lemmatize and self.lemmatizer:\n",
        "            filtered = [self.lemmatizer.lemmatize(t) for t in filtered]\n",
        "        return filtered\n",
        "\n",
        "    def preprocess_for_tfidf(self, text):\n",
        "        return ' '.join(self.preprocess(text))\n",
        "\n",
        "    def preprocess_for_embeddings(self, text):\n",
        "        return self.preprocess(text)\n",
        "\n",
        "preprocessor = TextPreprocessor()\n",
        "print(\"TextPreprocessor defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## TF-IDF, Skip-gram, CBOW (no external files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "class TFIDFEmbedding:\n",
        "    def __init__(self, max_features=10000, ngram_range=(1, 2), min_df=2, max_df=0.95):\n",
        "        self.vectorizer = TfidfVectorizer(max_features=max_features, ngram_range=ngram_range,\n",
        "                                          lowercase=False, min_df=min_df, max_df=max_df)\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, texts):\n",
        "        self.vectorizer.fit(texts)\n",
        "        self.is_fitted = True\n",
        "\n",
        "    def transform(self, texts):\n",
        "        return self.vectorizer.transform(texts).toarray()\n",
        "\n",
        "    def fit_transform(self, texts):\n",
        "        return self.vectorizer.fit_transform(texts).toarray()\n",
        "\n",
        "\n",
        "class SkipGramEmbedding:\n",
        "    def __init__(self, vector_size=300, window=5, min_count=2, workers=4, epochs=10):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.epochs = epochs\n",
        "        self.model = None\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, tokenized_texts):\n",
        "        self.model = Word2Vec(sentences=tokenized_texts, vector_size=self.vector_size,\n",
        "                               window=self.window, min_count=self.min_count,\n",
        "                               workers=self.workers, sg=1, epochs=self.epochs)\n",
        "        self.is_fitted = True\n",
        "\n",
        "    def transform(self, tokenized_texts):\n",
        "        embs = []\n",
        "        for tokens in tokenized_texts:\n",
        "            vecs = [self.model.wv[t] for t in tokens if t in self.model.wv]\n",
        "            embs.append(np.mean(vecs, axis=0) if vecs else np.zeros(self.vector_size))\n",
        "        return np.array(embs)\n",
        "\n",
        "\n",
        "class CBOWEmbedding:\n",
        "    def __init__(self, vector_size=300, window=5, min_count=2, workers=4, epochs=10):\n",
        "        self.vector_size = vector_size\n",
        "        self.window = window\n",
        "        self.min_count = min_count\n",
        "        self.workers = workers\n",
        "        self.epochs = epochs\n",
        "        self.model = None\n",
        "        self.is_fitted = False\n",
        "\n",
        "    def fit(self, tokenized_texts):\n",
        "        self.model = Word2Vec(sentences=tokenized_texts, vector_size=self.vector_size,\n",
        "                               window=self.window, min_count=self.min_count,\n",
        "                               workers=self.workers, sg=0, epochs=self.epochs)\n",
        "        self.is_fitted = True\n",
        "\n",
        "    def transform(self, tokenized_texts):\n",
        "        embs = []\n",
        "        for tokens in tokenized_texts:\n",
        "            vecs = [self.model.wv[t] for t in tokens if t in self.model.wv]\n",
        "            embs.append(np.mean(vecs, axis=0) if vecs else np.zeros(self.vector_size))\n",
        "        return np.array(embs)\n",
        "\n",
        "print(\"TFIDFEmbedding, SkipGramEmbedding, CBOWEmbedding defined.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Load and Explore Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the CSV file (correct full path)\n",
        "df = pd.read_csv(DATA_PATH)\n",
        "\n",
        "# Strip whitespace from column names\n",
        "df.columns = df.columns.str.strip()\n",
        "\n",
        "print(\"Dataset shape:\", df.shape)\n",
        "print(\"\\nColumns:\", df.columns.tolist())\n",
        "\n",
        "# Show first rows\n",
        "df.head()\n",
        "\n",
        "# Column names for rest of notebook\n",
        "text_col = 'Product Title'\n",
        "label_col = 'Cluster Label'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2. Preprocess"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_clean = df[[text_col, label_col]].dropna()\n",
        "texts_tfidf = [preprocessor.preprocess_for_tfidf(t) for t in df_clean[text_col]]\n",
        "texts_tokenized = [preprocessor.preprocess_for_embeddings(t) for t in df_clean[text_col]]\n",
        "print(f\"Samples: {len(texts_tfidf)}\")\n",
        "print(f\"Sample (TF-IDF): {texts_tfidf[0][:80]}...\")\n",
        "print(f\"Sample (tokens): {texts_tokenized[0][:10]}...\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3. Fit Embeddings (in-notebook only, no saved files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Slightly smaller/faster in FAST_MODE\n",
        "max_feat = 5000 if FAST_MODE else 10000\n",
        "w2v_epochs = 5 if FAST_MODE else 10\n",
        "tfidf_emb = TFIDFEmbedding(max_features=max_feat, ngram_range=(1, 2))\n",
        "skipgram_emb = SkipGramEmbedding(vector_size=300, window=5, min_count=2, epochs=w2v_epochs)\n",
        "cbow_emb = CBOWEmbedding(vector_size=300, window=5, min_count=2, epochs=w2v_epochs)\n",
        "\n",
        "print(\"Fitting TF-IDF...\")\n",
        "tfidf_emb.fit(texts_tfidf)\n",
        "print(\"Fitting Skip-gram...\")\n",
        "skipgram_emb.fit(texts_tokenized)\n",
        "print(\"Fitting CBOW...\")\n",
        "cbow_emb.fit(texts_tokenized)\n",
        "print(\"All embeddings fitted.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Train/Val/Test Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "y = label_encoder.fit_transform(df_clean[label_col])\n",
        "class_counts = Counter(y)\n",
        "min_samples_per_class = 2\n",
        "valid_indices = [i for i, label in enumerate(y) if class_counts[label] >= min_samples_per_class]\n",
        "\n",
        "if len(valid_indices) < len(y):\n",
        "    max_valid_idx = min(len(texts_tfidf), len(texts_tokenized), len(y))\n",
        "    valid_indices = [i for i in valid_indices if i < max_valid_idx]\n",
        "    texts_tfidf = [texts_tfidf[i] for i in valid_indices]\n",
        "    texts_tokenized = [texts_tokenized[i] for i in valid_indices]\n",
        "    y = [y[i] for i in valid_indices]\n",
        "    y = label_encoder.fit_transform(y)\n",
        "\n",
        "test_size = 0.15\n",
        "class_counts_after = Counter(y)\n",
        "num_classes = len(class_counts_after)\n",
        "min_class_count = min(class_counts_after.values())\n",
        "total_samples = len(y)\n",
        "min_test_samples = int(test_size * total_samples)\n",
        "can_stratify = (min_test_samples >= num_classes) and (min_class_count >= 2)\n",
        "\n",
        "if can_stratify:\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(range(len(texts_tfidf)), y, test_size=test_size, random_state=42, stratify=y)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15/0.85, random_state=42, stratify=y_temp)\n",
        "else:\n",
        "    X_temp, X_test, y_temp, y_test = train_test_split(range(len(texts_tfidf)), y, test_size=test_size, random_state=42)\n",
        "    X_train, X_val, y_train, y_val = train_test_split(X_temp, y_temp, test_size=0.15/0.85, random_state=42)\n",
        "\n",
        "texts_tfidf_train = [texts_tfidf[i] for i in X_train]\n",
        "texts_tfidf_val = [texts_tfidf[i] for i in X_val]\n",
        "texts_tfidf_test = [texts_tfidf[i] for i in X_test]\n",
        "texts_tokenized_train = [texts_tokenized[i] for i in X_train]\n",
        "texts_tokenized_val = [texts_tokenized[i] for i in X_val]\n",
        "texts_tokenized_test = [texts_tokenized[i] for i in X_test]\n",
        "\n",
        "# FAST_MODE: keep only top MAX_CLASSES and optionally cap train size\n",
        "if FAST_MODE and MAX_CLASSES is not None:\n",
        "    from collections import Counter\n",
        "    cnt = Counter(y_train)\n",
        "    top_classes = [c for c, _ in cnt.most_common(MAX_CLASSES)]\n",
        "    top_set = set(top_classes)\n",
        "    keep_train = [i for i in range(len(y_train)) if y_train[i] in top_set]\n",
        "    keep_val = [i for i in range(len(y_val)) if y_val[i] in top_set]\n",
        "    keep_test = [i for i in range(len(y_test)) if y_test[i] in top_set]\n",
        "    if MAX_TRAIN_SAMPLES and len(keep_train) > MAX_TRAIN_SAMPLES:\n",
        "        import random\n",
        "        random.seed(42)\n",
        "        keep_train = random.sample(keep_train, MAX_TRAIN_SAMPLES)\n",
        "    X_train = [X_train[i] for i in keep_train]\n",
        "    y_train = [y_train[i] for i in keep_train]\n",
        "    X_val = [X_val[i] for i in keep_val]\n",
        "    y_val = [y_val[i] for i in keep_val]\n",
        "    X_test = [X_test[i] for i in keep_test]\n",
        "    y_test = [y_test[i] for i in keep_test]\n",
        "    label_encoder = LabelEncoder()\n",
        "    y_train = label_encoder.fit_transform(y_train)\n",
        "    y_val = label_encoder.transform(y_val)\n",
        "    y_test = label_encoder.transform(y_test)\n",
        "    texts_tfidf_train = [texts_tfidf_train[i] for i in keep_train]\n",
        "    texts_tfidf_val = [texts_tfidf_val[i] for i in keep_val]\n",
        "    texts_tfidf_test = [texts_tfidf_test[i] for i in keep_test]\n",
        "    texts_tokenized_train = [texts_tokenized_train[i] for i in keep_train]\n",
        "    texts_tokenized_val = [texts_tokenized_val[i] for i in keep_val]\n",
        "    texts_tokenized_test = [texts_tokenized_test[i] for i in keep_test]\n",
        "    print(f\"FAST_MODE: Train {len(y_train)}, Val {len(y_val)}, Test {len(y_test)}, Classes {len(np.unique(y_train))}\")\n",
        "\n",
        "print(f\"Train: {len(X_train)}, Val: {len(X_val)}, Test: {len(X_test)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Transform Splits with Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train_tfidf = tfidf_emb.transform(texts_tfidf_train)\n",
        "X_val_tfidf = tfidf_emb.transform(texts_tfidf_val)\n",
        "X_test_tfidf = tfidf_emb.transform(texts_tfidf_test)\n",
        "\n",
        "X_train_skipgram = skipgram_emb.transform(texts_tokenized_train)\n",
        "X_val_skipgram = skipgram_emb.transform(texts_tokenized_val)\n",
        "X_test_skipgram = skipgram_emb.transform(texts_tokenized_test)\n",
        "\n",
        "X_train_cbow = cbow_emb.transform(texts_tokenized_train)\n",
        "X_val_cbow = cbow_emb.transform(texts_tokenized_val)\n",
        "X_test_cbow = cbow_emb.transform(texts_tokenized_test)\n",
        "\n",
        "print(f\"TF-IDF: {X_train_tfidf.shape}, Skip-gram: {X_train_skipgram.shape}, CBOW: {X_train_cbow.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Baseline SVM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "baseline_svm_tfidf = SVC(random_state=42, probability=False)\n",
        "baseline_svm_tfidf.fit(X_train_tfidf, y_train)\n",
        "y_val_pred_baseline_tfidf = baseline_svm_tfidf.predict(X_val_tfidf)\n",
        "print(f\"TF-IDF Val Accuracy: {accuracy_score(y_val, y_val_pred_baseline_tfidf):.4f}, F1: {f1_score(y_val, y_val_pred_baseline_tfidf, average='macro'):.4f}\")\n",
        "\n",
        "baseline_svm_skipgram = SVC(random_state=42, probability=False)\n",
        "baseline_svm_skipgram.fit(X_train_skipgram, y_train)\n",
        "y_val_pred_baseline_skipgram = baseline_svm_skipgram.predict(X_val_skipgram)\n",
        "print(f\"Skip-gram Val Accuracy: {accuracy_score(y_val, y_val_pred_baseline_skipgram):.4f}, F1: {f1_score(y_val, y_val_pred_baseline_skipgram, average='macro'):.4f}\")\n",
        "\n",
        "baseline_svm_cbow = SVC(random_state=42, probability=False)\n",
        "baseline_svm_cbow.fit(X_train_cbow, y_train)\n",
        "y_val_pred_baseline_cbow = baseline_svm_cbow.predict(X_val_cbow)\n",
        "print(f\"CBOW Val Accuracy: {accuracy_score(y_val, y_val_pred_baseline_cbow):.4f}, F1: {f1_score(y_val, y_val_pred_baseline_cbow, average='macro'):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Hyperparameter Tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# FAST_MODE: minimal grid (1 setting) + cv=2 for much faster tuning\n",
        "if FAST_MODE:\n",
        "    svm_param_grid = {'C': [1.0], 'kernel': ['linear'], 'class_weight': ['balanced']}\n",
        "    grid_cv_folds, use_probability = 2, False\n",
        "else:\n",
        "    svm_param_grid = {'C': [0.01, 0.1, 1.0, 10.0], 'kernel': ['linear', 'rbf'], 'gamma': ['scale', 'auto'], 'class_weight': [None, 'balanced']}\n",
        "    grid_cv_folds, use_probability = 3, True\n",
        "print(\"Param grid:\", svm_param_grid)\n",
        "print(f\"CV folds: {grid_cv_folds}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"GridSearch SVM + TF-IDF...\")\n",
        "\n",
        "svm_tfidf = GridSearchCV(\n",
        "    SVC(random_state=42, probability=use_probability),\n",
        "    param_grid=svm_param_grid,\n",
        "    cv=grid_cv_folds,\n",
        "    scoring='f1_macro',\n",
        "    n_jobs=-1,\n",
        "    verbose=1,\n",
        "    refit=True\n",
        ")\n",
        "\n",
        "svm_tfidf.fit(X_train_tfidf, y_train)\n",
        "\n",
        "# FIXED LINE\n",
        "print(f\"Best: {svm_tfidf.best_params_}  Score: {svm_tfidf.best_score_:.4f}\")\n",
        "\n",
        "y_val_pred_tfidf = svm_tfidf.predict(X_val_tfidf)\n",
        "\n",
        "print(f\"Val F1: {f1_score(y_val, y_val_pred_tfidf, average='macro'):.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"GridSearch SVM + Skip-gram...\")\n",
        "svm_skipgram = GridSearchCV(SVC(random_state=42, probability=use_probability), param_grid=svm_param_grid, cv=grid_cv_folds, scoring='f1_macro', n_jobs=-1, verbose=1, refit=True)\n",
        "svm_skipgram.fit(X_train_skipgram, y_train)\n",
        "print(\"Best:\", svm_skipgram.best_params_, \"Score:\", svm_skipgram.best_score_:.4f)\n",
        "y_val_pred_skipgram = svm_skipgram.predict(X_val_skipgram)\n",
        "print(f\"Val F1: {f1_score(y_val, y_val_pred_skipgram, average='macro'):.4f}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"GridSearch SVM + CBOW...\")\n",
        "svm_cbow = GridSearchCV(SVC(random_state=42, probability=use_probability), param_grid=svm_param_grid, cv=grid_cv_folds, scoring='f1_macro', n_jobs=-1, verbose=1, refit=True)\n",
        "svm_cbow.fit(X_train_cbow, y_train)\n",
        "print(\"Best:\", svm_cbow.best_params_, \"Score:\", svm_cbow.best_score_:.4f)\n",
        "y_val_pred_cbow = svm_cbow.predict(X_val_cbow)\n",
        "print(f\"Val F1: {f1_score(y_val, y_val_pred_cbow, average='macro'):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Test Set Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y_test_pred_tfidf = svm_tfidf.predict(X_test_tfidf)\n",
        "y_test_pred_skipgram = svm_skipgram.predict(X_test_skipgram)\n",
        "y_test_pred_cbow = svm_cbow.predict(X_test_cbow)\n",
        "\n",
        "results = {\n",
        "    'TF-IDF': {'accuracy': accuracy_score(y_test, y_test_pred_tfidf), 'f1_macro': f1_score(y_test, y_test_pred_tfidf, average='macro', zero_division=0)},\n",
        "    'Skip-gram': {'accuracy': accuracy_score(y_test, y_test_pred_skipgram), 'f1_macro': f1_score(y_test, y_test_pred_skipgram, average='macro', zero_division=0)},\n",
        "    'CBOW': {'accuracy': accuracy_score(y_test, y_test_pred_cbow), 'f1_macro': f1_score(y_test, y_test_pred_cbow, average='macro', zero_division=0)}\n",
        "}\n",
        "comparison_df = pd.DataFrame(results).T\n",
        "print(comparison_df.round(4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 2, figsize=(12, 4))\n",
        "metrics = ['accuracy', 'f1_macro']\n",
        "for i, m in enumerate(metrics):\n",
        "    vals = [results[emb][m] for emb in ['TF-IDF', 'Skip-gram', 'CBOW']]\n",
        "    ax[i].bar(['TF-IDF', 'Skip-gram', 'CBOW'], vals, color=['#3498db', '#e74c3c', '#2ecc71'])\n",
        "    ax[i].set_ylim(0, 1)\n",
        "    ax[i].set_title(m)\n",
        "plt.suptitle('SVM on Kaggle: TF-IDF vs Skip-gram vs CBOW')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
